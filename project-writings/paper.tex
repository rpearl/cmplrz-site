\documentclass{article}
\usepackage{fullpage}
%\usepackage{times}
\usepackage{url}
\usepackage{amsmath,amsthm,amssymb,color}
%\usepackage{multicol}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{courier}
\usepackage{listings}
%\usepackage[all]{hypcap}

\newcommand{\Squash}{\mathsf{Squash}}
\newcommand{\Clamp}{\mathsf{Clamp}}
\newcommand{\Range}{\mathsf{Range}}

\let\x\cdot
\lstset{
  language=Python,
  showstringspaces=false,
  tabsize=4,
  basicstyle=\ttfamily,
  keywordstyle=\bfseries\ttfamily,
  captionpos=b
}

\begin{document}

\title{\bf Range Analysis for the IonMonkey JavaScript engine}

\author{
Ryan Pearl\\
\texttt{$<$rpearl@andrew.cmu.edu$>$}\\
\and
Michael Sullivan\\
\texttt{$<$mjsulliv@cs.cmu.edu$>$}\\
}

\maketitle

\section{Introduction}
With the explosion of browser based applications that has occurred in
recent years, Javascript has become one of the most widely used
programming languages, and Javascript performance has become
critically important. Unfortunately, Javascript seems to be designed
to be un-performant.

One source of these woes are Javascript's dynamic semantics for
numbers: all numeric values are defined by the specification to be
IEEE 754 double precision floating point values (with bit-wise
operations being defined to round to the nearest 32 bit integer before
performing the operation). Since the common case is that numeric
values are integers and because floating point operations are
more expensive, all major Javascript implementations attempt to
represent numbers as integers when possible.

In IonMonkey, Mozilla's next-generation optimizing Javascript JIT
compiler, a hybrid type inference system (which uses both static analysis and
runtime information\cite{BHackettTI}) is used to generate
type-specialized code for the common case of integer arithmetic, and
type guards are inserted if the type inferencer is not certain. The
catch is that since double precision floats have a larger range than
32-bit integers, if the integer operations would overflow the result
must be promoted to being stored in a double. IonMonkey deals with
this by inserting overflow checks after arithmetic operations and
bailing out of the type specialized code if they trip. While this is
sometimes actually necessary, it is frequently the case that overflow
is not possible.

We have implemented integer value range analysis in the IonMonkey
engine and used it to eliminate provably unnecessary overflow checks.
We implemented an SSA-based worklist range analysis algorithm that
propagates integer ranges for variables by performing range
arithmetic. The algorithm is based off of Gough and Klaeren's range
analysis for bounds check elimination
\cite{Gough94eliminatingrange}. A range (including information about
possible overflow) is associated with each computation, and the ranges
are consulted to determine whether to perform overflow checks for
arithmetic computations.

OTHER OPTIMIZATIONS WE DO??

The core of our analysis system is essentially an implementation of
Gough and Klaeren's static range analysis algorithm for bounds check
elimination.
We identify our technical contributions as:
\begin{itemize}
\item An implementation of Gough and Klaeren's range analysis in a
  modern industrial-strength SSA-based compiler.
\item A different algorithm for inserting and removing $\beta$ nodes
  that used by Gough and Klaeren that takes advantage of the data
  structures of a modern SSA compiler.
\item To facilitate the above, extended IonMonkey with a fast
  (constant time, one integer per basic block memory overhead) 
  mechanism for determining whether a block is dominated by another.
\item Extending the analysis to produce the information needed for
  overflow check elimination by differentiating between
  ``maximum/minimum 32-bit int values'' and ``infinity'' (possible
  overflow).
\item The observation that when doing range computation with the above
  distinction, the input ranges can be always be ``squashed'' to only
  finite ranges, since if overflow had occurred, the code would have
  bailed out.
\item An implementation of overflow check elimination using the above
  analyses.
\end{itemize}


\section{Related Work}
%% XXX: FIXME, TODO
There are a variety of approaches to range analysis. A recent work, ``The Design
and Implementation of a Non-Iterative Range Analysis Algorithm on a
Production Compiler'', by Teixeira and Pereira, outlines a practical implementation of
implementation of an algorithm designed by by Su and Wagner
\cite{Su04aclass}. The algorithm due to Su and Wagner is
non-iterative, and works by constructing a constraint graph from the program's control flow graph.

A variety of other range analyses are available which formulate the problem as
a more traditional iterative dataflow problem. GCC, for example, uses an SSA
workflow algorithm due to Patterson in ``Accurate Static Branch Prediction by
Value Range Propagation.'' The orignal non-SSA based dataflow algorithm is by
Harrison in ``Compiler Analysis of the Value Ranges of Variables.''

We reviewed the literature on different types of range analysis and
decided to implement an analysis based on Gough and Klaeren's SSA
based range analysis algorithm \cite{Gough94eliminatingrange}.

\section{IonMonkey Overview}
The IonMonkey compiler is somewhat loosely based on LLVM, in that it is an
imperative SSA compiler. IonMonkey takes the bytecode used by the SpiderMonkey
Javascript interpreter and compiles it to native code. Optimistic type
specialization is used, and if type assumptions fail, then execution ``bails
out'' to the interpreter.

IonMonkey has two intermediate languages. There is a ``Middle
Intermediate Representation'' (MIR) which encompasses higher-level
constructs such as lambdas, objects, and strings. A variety of
optimizations are performed, and then the code is lowered to a ``Lower
Intermediate Representation'' (LIR) used for register allocation and
code generation.

While the ECMA specification for Javascript requires that all numbers behave as
double-precision floating point values, for performance reasons JIT compilers
represent numbers as 32-bit integers when possible. However, since doubles can
represent larger values than integers, operations must be guarded against
overflow.  Our range analysis pass eliminates overflow checks when operations
can be proven not to overflow.

\section{Implementation}

\subsection{Ranges and Range Arithmetic}

In Single Static Assignment form, each computation has a distinct
name. We take advantage of this to assign each name (that is, each
computation) a ``range''; the ranges in our system are a contiguous
subset of the set ${-\infty} \cup [-2^{31}, 2^{31}-1] \cup {\infty}$
that represent the possible results a computation may have. A range
that includes $\pm \infty$ indicates that the associated
computation may overflow in that direction. This distinction between
the minimum/maximum 32-bit integer values and $\pm \infty$ is the key
to detecting possible overflow.

\begin{figure}[ht]
\begin{eqnarray*}
Z &=& {-\infty} \cup [-2^{31}, 2^{31}-1] \cup {\infty} \\
\Clamp(n) &=& \begin{cases}
-\infty &\text{if } n < -2^{31} \\
\infty &\text{if } n > -2^{31}-1 \\
n&\text{otherwise } \\
\end{cases}\\
\Clamp([l, h]) &=& [\Clamp(l), \Clamp(h)] \\
{[x_l, x_h] \cup [y_l, y_h]} &=& [\min(x_l, y_l), \max(x_h, y_h)] \\
{[x_l, x_h] \cap [y_l, y_h]} &=& [\max(x_l, y_l), \min(x_h, y_h)] \\
{[x_l, x_h] + [y_l, y_h]} &=& \Clamp ([x_l + y_l, x_h + y_h]) \\
{[x_l, x_h] - [y_l, y_h]} &=& \Clamp ([x_l - y_h, x_h + y_l]) \\
{[x_l, x_h] \x [y_l, y_h]} &=& 
\begin{tabular}{rl}
$\Clamp ([$&$\min(x_l \x y_l, x_l \x y_h, x_h \x y_l, x_h \x y_h),$\\
&$\max(x_l \x y_l, x_l \x y_h, x_h \x y_l, x_h \x y_h)])$ \\
\end{tabular}
\end{eqnarray*}

\caption{Range arithmetic definitions}
\label{fig:range_arith}
\end{figure}

We can define arithmetical and set operations on these ranges in the
standard way, ``clamping'' values outside the range of a 32-bit signed
integer to $\pm \infty$. Some definitions for this are are shown in
Figure \ref{fig:range_arith}.

\subsection{Conditional Branches and $\beta$ nobes}

An important source of range information that requires care to take
advantage of is conditional branches. Consider the code in Figure
\ref{fig:branch}. All three arithmetic operations in this code can be
proven to not overflow, but to do this it is not sufficient to simply
associate each name with a range, since the information differs
between basic blocks. An obvious approach is to instead associate
ranges with name/basic block pairs, allowing representation of
flow-sensitive range information. This solution, however, is
unsatisfying. One of the major benefits of SSA-style analysis is
eliminating the need for this sort of tracking; by giving each
computation a unique name, SSA reduces the need for tracking
information about variables across time; adopting this approach would
undo this.

The approach we took, again following \cite{Gough94eliminatingrange},
is to instead attack the problem in ``SSA style'', by introducing
$\beta$-nodes, a new form of pseudo-operation that associates range
information with a value. $\beta$ nodes take one argument and
additionally have an auxiliary constant range associated with
them. Operationally, $\beta$-nodes are simply copies, but the
invariant is maintained that the output value will fall inside the
range given by the $\beta$ node. Gough and Klaeren refer to SSA
extended with $\beta$ nodes as XSA.

\begin{figure}[ht]
\begin{center}
\begin{verbatim}
var y;
if (x < 0) {
    y = x + 2000000000;
} else {
    if (x < 1000000000) {
        y = x*2;
    } else {
        y = x - 3000000000;
    }
}
\end{verbatim}
\end{center}
\caption{Code that requires information about branches to properly analyze}
\label{fig:branch}
\end{figure}

\begin{figure}[ht]
\begin{center}
\begin{verbatim}
if (x < 0) {
    x1 = Beta(x, [INT_MIN, -1]);
    y1 = x1 + 2000000000;
} else {
    x2 = Beta(x, [0, INT_MAX]);
    if (x2 < 1000000000) {
        x3 = Beta(x2, [INT_MIN, 999999999]);
        y2 = x3*2;
    } else {
        x4 = Beta(x2, [1000000000, INT_MAX]);
        y3 = x4 - 3000000000;
    }
    y4 = Phi(y2, y3);
}
y = Phi(y1, y4);
\end{verbatim}
\end{center}
\caption{Code from Figure \ref{fig:branch}, in XSA form}
\label{fig:xsa}
\end{figure}

\subsection{Inserting and Removing Beta Nodes}
To put programs in XSA form, Gough and Klaeren presented an extension
to Cytron et al.'s standard algorithm for computing SSA form using
dominance frontiers \cite{Cytron:1991:ECS:115372.115320}.

This approach does not work for us for two reasons: first, IonMonkey
does not use Cytron et al.'s algorithm for computing SSA form; it
instead takes advantage of the structured nature of Javascript code to
go to SSA form without computing dominance frontiers \footnote{More
  precisely, it takes advantage in an ad-hoc and terrifying way of
  poorly documented structured invariants and ``source notes'' in the
  bytecode that the SpiderMonkey front-end emits.}.  Furthermore, we
decided that instead of putting the program into XSA form at the
beginning and leaving it there through the compilation phase, it would
be a better idea to introduce $\beta$ nodes solely for performing
range analysis and remove them afterwards. This is for two main
reasons: first, as a question of engineering effort, we did not want
to teach every pass of the compiler how to deal with $\beta$ nodes,
and the IonMonkey team does not want us to change the IL in a pervasive
way simply for range analysis; second, $\beta$ nodes are copies that
are not allowed to be propagated away, which may inhibit optimization
passes (like global value numbering).

Thus we developed a quite simple and in practice efficient algorithm to
add $\beta$ nodes to an SSA source program by taking
advantage of data structures common to modern SSA based compilers
\footnote{The method for removing $\beta$ nodes is so trivial that we
  could not claim with a straight face that we ``developed an
  algorithm'' to perform it.} .  Pseudocode for the algorithm is shown
in Listing \ref{lst:add_betas}. The algorithm traverses the control
flow group, looking for blocks that have been entered by a jump
conditioned on a relational operation. When we find a variable being
compared to a constant, we compute a range and use it to create a new
$\beta$ node. We want to propagate this range information to all uses
of the variable where the branch is guaranteed to have been taken;
this is accomplished by replacing all uses of original instruction
that are dominated by the new $\beta$ instruction with the $\beta$. To
do this, we take advantage of IonMonkey's use chains to efficiently
iterate over all uses of an instruction.

In order to do this, we needed to be able to efficiently determine
whether one block is dominated by another. The best way to do that
that existed in IonMonkey was to walk up the dominator tree looking
for the block being tested against. We felt that it would be too
inefficient to need to do this for every use of every variable tested
with a conditional. To do this, we annotated each basic block with its
index in a pre-order traversal of the dominator tree. Since IonMonkey
basic blocks already tracked how many other blocks they dominated, we
can now easily check whether a block dominates another by checking
whether the potential descendant's index falls within the range of the
ancestor's dominated blocks.

\begin{lstlisting}[language=Python,
                   caption={Pseudocode algorithm for inserting beta nodes},
                   label={lst:add_betas}]
def addBetaNodesToFunc(cfg):
  for b in cfg.basic_blocks():
    if b.numPredecessors() == 1 and isBranch(b.pred(0).lastInstruction()):
      test = b.pred(0).lastInstruction()
      # find whether we are the true or false branch
      direction = (b == test.trueTarget())
      if test.op() in RELOPs:
          r = computeRange(test.op(), c, direction)
          x_prime = b.insertFront(Beta(x, r))
          replaceDominatedUses(x, x_prime)

# Replace all uses of the instruction x that are dominated by
# the instruction x_prime with x_prime
def replaceDominatedUses(x, x_prime):
  for use in x.uses():
    ins = use.instruction()
    if ins != x_prime and isDominatedUse(x_prime, use):
      replaceUseWith(use, x_prime)

def isDominatedUse(x_prime, use):
  ins = use.instruction()
  if ins.isPhiNode():
    # Get the predecessor block that corresponds to the
    # argument of the phi node we are looking at
    phi_arg_pred_block = ins.block().pred(use.index())
    return blockDominates(x_prime.block(), phi_arg_pred_block)
  else:
    return blockDominates(x_prime.block(), x.block())

def blockDominates(b1, b2):
  high = b1.domIndex() + b1.numDominated()
  low  = b1.domIndex()
  return b2.domIndex() >= low and b2.domIndex() <= high
\end{lstlisting}

The method for removing beta nodes is trivial. Traverse the program
removing beta nodes and replacing all uses of them with their
arguments. Since beta nodes only are placed at the beginning of basic
blocks, we only need to iterate through the $\beta$ node prefix of a
block.

\subsection{Range Analysis}

At the start of range analysis, all ranges are initialized to
$[-\infty, \infty]$.

XXX: rpearl, explain how the analysis pass works; specifically, when
we add things to the worklist, because that is different from the
paper. maybe add pseudocode

Recomputation of the possible range of an instruction is fairly simple
and proceeds by the rules given in Figure \ref{fig:range_anal}. The
key insight we had in these equations is that while the ranges of the
inputs to an operation may be infinite (that is, they may have
overflowed), this possibility can be ignored when using the ranges as
inputs. This is because had the computation overflowed, it would have
been detected and IonMonkey would bail out to non-integer code. We
know that if code that assumes the input is an integer is still being
run, then the operation must not have overflowed, and thus must have a
finite range. What this means for the range analysis equations is that
we can always ``squash'' the ranges of inputs by replacing positive
and negative infinite with the maximum and minimum 32-bit integers,
respectively.

\begin{figure}[ht]
\begin{eqnarray*}
\Squash(n) &=& \begin{cases}
 -2^{31} &\text{if } n = -\infty \\
2^{31}-1 &\text{if } n = \infty \\
n&\text{otherwise } \\
\end{cases}\\
\Squash([l, h]) &=& [\Squash(l), \Squash(h)]
\end{eqnarray*}
%
\begin{eqnarray*}
x = \Phi(y, z) &\Rightarrow&
    \Range(x) = \Squash(\Range(y)) \cup \Squash(\Range(z)) \\
x = \beta(y, [l, h]) &\Rightarrow& \Range(x) = \Squash(\Range(y)) \cap [l, h] \\
x = y + z &\Rightarrow& \Range(x) = \Squash(\Range(y)) + \Squash(\Range(z)) \\
x = y - z &\Rightarrow& \Range(x) = \Squash(\Range(y)) - \Squash(\Range(z)) \\
x = y \x z &\Rightarrow& \Range(x) = \Squash(\Range(y)) \x \Squash(\Range(z)) \\
x = c &\Rightarrow& \Range(x) = [c, c]
\end{eqnarray*}
\caption{Range analysis equations}
\label{fig:range_anal}
\end{figure}

\subsection{Overflow Check Elimination}
Overflow check elimination is implemented as part of the lowering pass
that generates the lower level intermediate representation (LIR) from
the middle intermediate representation (MIR). During lowering,
operations that can fail in such a way as to require a bailout have
``snapshots'' associated with them. During actual code generation,
instruction-specific checking code (overflow checking, in this case)
is emitted for operations that can fail and have snapshots associated
with them.

During lowering of arithmetic operations that can overflow, we check
whether the instructions range is finite; if it is, we supress
assigning a snapshot to the operation, eliminating the overflow check.

\subsection{Array Bounds Check Elimination}
We do not perform general array bounds check elimination. To do this
fully would require symbolic range analysis. Since IonMonkey already
does hoisting of bounds checks out of loops, we determined that the
effort would probably not be worth the gain.

We do eliminate bounds checks in one situation: when bounds checks
have been hoisted out of a loop, a check is added that specifically
bounds checks whether a value is greater than the array minimum index
(which, unfortunately, need not be zero). During lowering we will
eliminate this check if we can prove that the index is greater than
the minimum value.

\section{Evaluation}
%% XXX: TODO

\section{Surprises and Lessons Learned}
%% XXX: TODO

\section{Conclusions}
%% XXX: TODO
Boy, JS is a pain in the ass.

\bibliography{citations}{}
\bibliographystyle{abbrv}


\end{document}
